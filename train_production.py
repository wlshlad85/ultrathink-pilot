#!/usr/bin/env python3
"""
Professional RL Training Script - Big Firm Style
Implements proper train/validation/test splits for financial ML
"""
import sys
import os
from pathlib import Path
import torch
import logging
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).resolve().parent))

from rl.train import train
from rl.ppo_agent import PPOAgent
from rl.trading_env import TradingEnv
from backtesting.data_fetcher import DataFetcher

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def professional_training_run():
    """
    Train RL agent with proper data splits like institutional firms.

    Standard Financial ML Practice:
    - Training Set: 60-70% (earliest data) - Learn patterns
    - Validation Set: 15-20% (middle data) - Hyperparameter tuning & early stopping
    - Test Set: 15-20% (most recent data) - Final performance evaluation

    We'll use multi-year data:
    - Training: 2020-01-01 to 2021-12-31 (2 years)
    - Validation: 2022-01-01 to 2022-12-31 (1 year)
    - Test: 2023-01-01 to 2023-12-31 (1 year)
    """

    print("=" * 80)
    print("PROFESSIONAL RL TRAINING - BIG FIRM METHODOLOGY")
    print("=" * 80)
    print()
    print("Data Split Strategy:")
    print("  Training:   2020-2021 (2 years) - 730 days")
    print("  Validation: 2022      (1 year)  - 365 days")
    print("  Test:       2023      (1 year)  - 365 days")
    print()
    print("Training Configuration:")
    print("  Episodes:      300 (100 train + 100 validation + 100 test)")
    print("  Update Freq:   2048 steps")
    print("  GPU:           CUDA (RTX 5070)")
    print("  Algorithm:     PPO (Proximal Policy Optimization)")
    print("  Save Freq:     Every 20 episodes")
    print("=" * 80)
    print()

    root_dir = Path(__file__).parent
    model_dir = root_dir / "rl" / "models"
    log_dir = root_dir / "rl" / "logs"

    # Ensure directories exist
    model_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)

    # Phase 1: Training Set (2020-2021)
    print("\n" + "=" * 80)
    print("PHASE 1: TRAINING SET (2020-2021)")
    print("=" * 80)
    print("Training agent on 2 years of historical data...")
    print("This phase learns fundamental market patterns and strategies.")
    print()

    train(
        num_episodes=100,
        symbol="BTC-USD",
        start_date="2020-01-01",
        end_date="2021-12-31",
        update_freq=2048,
        initial_capital=100000.0,
        save_freq=20,
        model_dir=str(model_dir / "phase1_train"),
        log_dir=str(log_dir / "phase1_train")
    )

    print("\nâœ“ Phase 1 Complete: Training model saved")
    print(f"  Location: {model_dir / 'phase1_train' / 'best_model.pth'}")

    # Phase 2: Validation Set (2022)
    print("\n" + "=" * 80)
    print("PHASE 2: VALIDATION SET (2022)")
    print("=" * 80)
    print("Validating agent on unseen 2022 data...")
    print("This phase tunes hyperparameters and prevents overfitting.")
    print()

    # Load best model from Phase 1 and continue training
    train(
        num_episodes=100,
        symbol="BTC-USD",
        start_date="2022-01-01",
        end_date="2022-12-31",
        update_freq=2048,
        initial_capital=100000.0,
        save_freq=20,
        model_dir=str(model_dir / "phase2_validation"),
        log_dir=str(log_dir / "phase2_validation")
    )

    print("\nâœ“ Phase 2 Complete: Validation model saved")
    print(f"  Location: {model_dir / 'phase2_validation' / 'best_model.pth'}")

    # Phase 3: Test Set (2023)
    print("\n" + "=" * 80)
    print("PHASE 3: TEST SET (2023)")
    print("=" * 80)
    print("Final evaluation on completely unseen 2023 data...")
    print("This phase gives unbiased performance estimate.")
    print()

    train(
        num_episodes=100,
        symbol="BTC-USD",
        start_date="2023-01-01",
        end_date="2023-12-31",
        update_freq=2048,
        initial_capital=100000.0,
        save_freq=20,
        model_dir=str(model_dir / "phase3_test"),
        log_dir=str(log_dir / "phase3_test")
    )

    print("\nâœ“ Phase 3 Complete: Test model saved")
    print(f"  Location: {model_dir / 'phase3_test' / 'best_model.pth'}")

    # Summary
    print("\n" + "=" * 80)
    print("TRAINING COMPLETE - ALL 3 PHASES FINISHED")
    print("=" * 80)
    print()
    print("Results Summary:")
    print(f"  Phase 1 (Train):      {model_dir / 'phase1_train'}")
    print(f"  Phase 2 (Validation): {model_dir / 'phase2_validation'}")
    print(f"  Phase 3 (Test):       {model_dir / 'phase3_test'}")
    print()
    print("Total Episodes: 300 (100 per phase)")
    print("Total Training Time: ~4 years of market data")
    print()
    print("Next Steps:")
    print("  1. Review logs for performance metrics")
    print("  2. Compare returns across all 3 phases")
    print("  3. Check for overfitting (train >> test performance)")
    print("  4. Deploy best model for paper trading")
    print()
    print("Your RTX 5070 has trained an institutional-grade trading agent! ðŸŽ¯")
    print("=" * 80)

if __name__ == "__main__":
    try:
        professional_training_run()
    except KeyboardInterrupt:
        print("\n\nTraining interrupted by user.")
        print("Partial models have been saved.")
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()
