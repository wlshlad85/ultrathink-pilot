#!/usr/bin/env python3
"""
Comprehensive synthetic data validation for regime-aware trading agent.

This script:
1. Calibrates synthetic generator on real training data (2017-2021)
2. Generates 6 test scenarios covering different market conditions
3. Tests the trained model on each synthetic scenario
4. Produces detailed analysis of strengths and weaknesses

The goal: Validate that the model learned transferable regime-conditional
patterns, not just memorized specific historical sequences.
"""

import sys
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from collections import defaultdict
from typing import Dict, List

sys.path.insert(0, str(Path(__file__).parent))

from rl.synthetic_market_generator import SyntheticMarketGenerator
from rl.trading_env_v2 import TradingEnvV2
from rl.ppo_agent import PPOAgent
from backtesting.data_fetcher import DataFetcher


def test_on_synthetic_scenario(
    agent,
    device,
    scenario_df: pd.DataFrame,
    scenario_name: str
) -> Dict:
    """
    Test agent on a single synthetic scenario.

    Args:
        agent: Trained PPO agent
        device: torch device
        scenario_df: Synthetic market data
        scenario_name: Name of scenario for reporting

    Returns:
        Dictionary of performance metrics
    """
    print(f"\n{'='*80}")
    print(f"SCENARIO: {scenario_name}")
    print(f"{'='*80}")

    # Save synthetic data temporarily
    temp_path = Path('temp_synthetic_data.csv')
    scenario_df.to_csv(temp_path, index=False)

    # Create custom environment from synthetic data
    # We'll need to modify this to load from our CSV
    # For now, let's manually create the environment

    # Simulate trading on synthetic data
    state = None

    # Initialize tracking
    actions_by_regime = defaultdict(lambda: {'HOLD': 0, 'BUY': 0, 'SELL': 0})
    regime_counts = defaultdict(int)
    action_probs_by_regime = defaultdict(list)
    returns_by_action = {'HOLD': [], 'BUY': [], 'SELL': []}

    portfolio_value = 100000.0
    cash = 100000.0
    position = 0.0
    portfolio_history = [portfolio_value]

    for idx in range(len(scenario_df)):
        row = scenario_df.iloc[idx]

        # Build state vector (matching TradingEnvV2 format)
        # We need to construct the state from the synthetic data
        # This is a simplified version - in practice we'd want to match exact state format

        if idx < 200:  # Not enough history for full state
            continue

        # Extract features
        price_data = scenario_df.iloc[max(0, idx-199):idx+1]

        # Build state (simplified - should match TradingEnvV2.get_state())
        close_prices = price_data['close'].values[-50:]
        returns = np.diff(close_prices) / close_prices[:-1]

        # Normalized features (example - should match exact state format)
        state = np.array([
            # Price momentum
            (close_prices[-1] / close_prices[-10] - 1),
            (close_prices[-1] / close_prices[-30] - 1),

            # Moving averages
            (close_prices[-1] / np.mean(close_prices[-10:]) - 1),
            (close_prices[-1] / np.mean(close_prices[-50:]) - 1),

            # Volatility
            np.std(returns),

            # RSI
            row['rsi_14'] / 100 - 0.5,

            # Regime encoding (this is key!)
            1.0 if row['regime'] == 'BULL' else 0.0,
            1.0 if row['regime'] == 'BEAR' else 0.0,
            1.0 if row['regime'] == 'SIDEWAYS' else 0.0,

            # Portfolio state
            position / 10,  # Normalized position
            cash / 100000 - 1,  # Normalized cash
        ], dtype=np.float32)

        # Get action from agent
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            action_probs, _ = agent.policy(state_tensor)
            action = torch.argmax(action_probs, dim=1).item()
            probs = action_probs[0].cpu().numpy()

        # Execute trade (simplified)
        action_name = ['HOLD', 'BUY', 'SELL'][action]
        regime = row['regime']
        current_price = row['close']

        if action == 1:  # BUY
            if cash >= 1000:  # Min trade size
                buy_amount = min(1000, cash)
                shares = buy_amount / current_price
                position += shares
                cash -= buy_amount
        elif action == 2:  # SELL
            if position > 0:
                sell_value = position * current_price
                cash += sell_value
                position = 0

        # Update portfolio value
        portfolio_value = cash + (position * current_price)
        portfolio_history.append(portfolio_value)

        # Record metrics
        actions_by_regime[regime][action_name] += 1
        regime_counts[regime] += 1
        action_probs_by_regime[regime].append(probs)

        prev_value = portfolio_history[-2]
        step_return = ((portfolio_value - prev_value) / prev_value) * 100
        returns_by_action[action_name].append(step_return)

    # Calculate final metrics
    final_value = portfolio_history[-1]
    total_return = ((final_value - 100000) / 100000) * 100

    # Sharpe ratio
    portfolio_returns = np.diff(portfolio_history) / portfolio_history[:-1]
    mean_return = np.mean(portfolio_returns)
    std_return = np.std(portfolio_returns)
    sharpe_ratio = (mean_return / std_return) * np.sqrt(252) if std_return > 0 else 0.0

    # Max drawdown
    running_max = np.maximum.accumulate(portfolio_history)
    drawdowns = (np.array(portfolio_history) - running_max) / running_max
    max_drawdown = np.min(drawdowns) * 100

    total_steps = sum(regime_counts.values())

    # Print results
    print(f"\nüìä PERFORMANCE:")
    print(f"  Final value:    ${final_value:,.2f}")
    print(f"  Total return:   {total_return:+.2f}%")
    print(f"  Sharpe ratio:   {sharpe_ratio:+.3f}")
    print(f"  Max drawdown:   {max_drawdown:.2f}%")
    print(f"  Total steps:    {total_steps}")

    print(f"\nüéØ REGIME DISTRIBUTION:")
    for regime in ['BULL', 'BEAR', 'SIDEWAYS']:
        if regime in regime_counts:
            pct = (regime_counts[regime] / total_steps) * 100
            print(f"  {regime:10s}: {regime_counts[regime]:4d} steps ({pct:5.1f}%)")

    print(f"\nü§ñ ACTIONS BY REGIME:")
    for regime in ['BULL', 'BEAR', 'SIDEWAYS']:
        if regime in actions_by_regime:
            actions = actions_by_regime[regime]
            total = sum(actions.values())
            print(f"\n  {regime}:")
            for action in ['HOLD', 'BUY', 'SELL']:
                count = actions[action]
                pct = (count / total * 100) if total > 0 else 0
                print(f"    {action:4s}: {count:4d} ({pct:5.1f}%)")

    print(f"\nüìà AVERAGE ACTION PROBABILITIES:")
    for regime in ['BULL', 'BEAR', 'SIDEWAYS']:
        if regime in action_probs_by_regime and len(action_probs_by_regime[regime]) > 0:
            avg_probs = np.mean(action_probs_by_regime[regime], axis=0)
            print(f"\n  {regime}:")
            print(f"    HOLD: {avg_probs[0]:.3f}")
            print(f"    BUY:  {avg_probs[1]:.3f}")
            print(f"    SELL: {avg_probs[2]:.3f}")

    # Clean up
    if temp_path.exists():
        temp_path.unlink()

    return {
        'scenario_name': scenario_name,
        'final_value': final_value,
        'total_return': total_return,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown,
        'actions_by_regime': dict(actions_by_regime),
        'regime_counts': dict(regime_counts),
        'buy_prob_bull': avg_probs[1] if 'BULL' in action_probs_by_regime else 0,
        'buy_prob_bear': np.mean([p[1] for p in action_probs_by_regime.get('BEAR', [[0, 0, 0]])]),
    }


def main():
    """Main validation pipeline."""

    print("="*80)
    print("SYNTHETIC DATA VALIDATION")
    print("="*80)
    print()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}\n")

    # Load trained model
    model_path = Path("rl/models/regime_aware_v2/best_model.pth")

    if not model_path.exists():
        print(f"‚ùå Model not found at {model_path}")
        return

    print(f"Loading model from {model_path}")

    # Create agent (dimensions from training)
    dummy_env = TradingEnvV2(
        symbol="BTC-USD",
        start_date="2017-01-01",
        end_date="2017-01-31",
        initial_capital=100000.0,
        use_regime_rewards=True
    )

    state_dim = dummy_env.observation_space.shape[0]
    action_dim = dummy_env.action_space.n

    agent = PPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        device=device
    )

    agent.load(str(model_path))
    print("‚úÖ Model loaded successfully\n")

    # Initialize synthetic data generator
    generator = SyntheticMarketGenerator(seed=42)

    # Calibrate from real training data
    print("="*80)
    print("CALIBRATION: Learning real market statistics")
    print("="*80)

    fetcher = DataFetcher(symbol="BTC-USD")
    real_data = fetcher.fetch(
        start_date="2017-01-01",
        end_date="2021-12-31"
    )
    real_data = fetcher.add_technical_indicators()

    # Add regime classification to real data for calibration
    from rl.regime_classifier import RegimeClassifier
    classifier = RegimeClassifier()
    real_data = classifier.classify(real_data)

    generator.calibrate_from_real_data(real_data)

    # Validate realism
    print("\n" + "="*80)
    print("REALISM VALIDATION: Testing synthetic data quality")
    print("="*80)

    test_synthetic = generator.generate_scenario(
        scenario_name='mixed_realistic',
        total_days=500
    )

    validation_metrics = generator.validate_realism(test_synthetic, real_data)

    print("\nStatistical Comparison (Synthetic vs Real):")
    print(f"  Return kurtosis:     {validation_metrics['synthetic_kurtosis']:.2f} vs {validation_metrics['real_kurtosis']:.2f}")
    print(f"  Return skewness:     {validation_metrics['synthetic_skewness']:.2f} vs {validation_metrics['real_skewness']:.2f}")
    print(f"  Volatility:          {validation_metrics['synthetic_volatility']:.4f} vs {validation_metrics['real_volatility']:.4f}")
    print(f"  Vol clustering:      {validation_metrics['synthetic_vol_clustering']:.3f} vs {validation_metrics['real_vol_clustering']:.3f}")
    print(f"  KS test p-value:     {validation_metrics['ks_pvalue']:.4f} (>0.05 = realistic)")

    # Generate and test scenarios
    print("\n" + "="*80)
    print("SYNTHETIC SCENARIO TESTING")
    print("="*80)

    scenarios_to_test = [
        ('extended_regimes', 910),  # Long pure regimes
        ('rapid_switching', 360),  # Frequent regime changes
        ('adversarial_bull_trap', 205),  # Bull trap crash
        ('black_swan', 367),  # Extreme crash event
        ('mixed_realistic', 815),  # Realistic mixed
    ]

    all_results = []

    for scenario_name, total_days in scenarios_to_test:
        print(f"\n{'='*80}")
        print(f"Generating scenario: {scenario_name}")
        print(f"{'='*80}")

        synthetic_df = generator.generate_scenario(
            scenario_name=scenario_name,
            total_days=total_days,
            initial_price=10000.0
        )

        print(f"Generated {len(synthetic_df)} days")
        print(f"Price: ${synthetic_df['close'].iloc[0]:.2f} ‚Üí ${synthetic_df['close'].iloc[-1]:.2f}")
        print(f"Regimes: {dict(synthetic_df['regime'].value_counts())}")

        # Test agent on this scenario
        result = test_on_synthetic_scenario(
            agent=agent,
            device=device,
            scenario_df=synthetic_df,
            scenario_name=scenario_name
        )

        all_results.append(result)

    # Final summary
    print("\n" + "="*80)
    print("VALIDATION SUMMARY")
    print("="*80)

    results_df = pd.DataFrame(all_results)

    print("\nPerformance Across All Scenarios:")
    print(results_df[['scenario_name', 'total_return', 'sharpe_ratio', 'max_drawdown']].to_string(index=False))

    print("\n\nBehavioral Consistency:")
    print("\nBUY Probability by Regime:")
    print(results_df[['scenario_name', 'buy_prob_bull', 'buy_prob_bear']].to_string(index=False))

    # Overall assessment
    print("\n" + "="*80)
    print("ASSESSMENT")
    print("="*80)

    avg_sharpe = results_df['sharpe_ratio'].mean()
    avg_return = results_df['total_return'].mean()
    consistency = results_df['buy_prob_bull'].mean() - results_df['buy_prob_bear'].mean()

    print(f"\nAverage Sharpe across scenarios: {avg_sharpe:+.3f}")
    print(f"Average return across scenarios: {avg_return:+.2f}%")
    print(f"Behavioral consistency (bull BUY - bear BUY): {consistency:.3f}")

    if avg_sharpe > 0.3 and consistency > 0.5:
        print("\n‚úÖ STRONG VALIDATION:")
        print("   Model demonstrates consistent regime-conditional behavior")
        print("   across diverse synthetic scenarios.")
        print("   ‚Üí Ready for paper trading")
    elif avg_sharpe > 0 and consistency > 0.3:
        print("\n‚ö†Ô∏è  MODERATE VALIDATION:")
        print("   Model shows some regime awareness but performance varies.")
        print("   ‚Üí Consider additional training or ensemble approach")
    else:
        print("\n‚ùå WEAK VALIDATION:")
        print("   Model fails to generalize to synthetic scenarios.")
        print("   ‚Üí Likely overfit to training data, needs redesign")


if __name__ == "__main__":
    main()
