import json

# Load all three experiment results
with open('rl/models/exp1_strong/training_metrics.json') as f:
    exp1 = json.load(f)
with open('rl/models/exp2_exp/training_metrics.json') as f:
    exp2 = json.load(f)
with open('rl/models/exp3_sharpe/training_metrics.json') as f:
    exp3 = json.load(f)

def mean(lst):
    return sum(lst) / len(lst)

def std(lst):
    m = mean(lst)
    variance = sum((x - m) ** 2 for x in lst) / len(lst)
    return variance ** 0.5

def count_positive(lst):
    return sum(1 for x in lst if x > 0)

print('='*80)
print('COMPREHENSIVE COMPARISON: THREE VOLATILITY-CONTROLLED REWARD SYSTEMS')
print('='*80)
print()

# Validation Sharpe (PRIMARY METRIC)
print('PRIMARY METRIC: VALIDATION SHARPE RATIO')
print('-' * 80)
val1 = exp1['best_val_sharpe']
val2 = exp2['best_val_sharpe']
val3 = exp3['best_val_sharpe']
print(f'  Exp1 (Strong Linear Penalty):      {val1:.3f}')
print(f'  Exp2 (Exponential Penalty):        {val2:.3f}')
print(f'  Exp3 (Direct Sharpe Optimization): {val3:.3f}')
print()
print(f'  üèÜ WINNER: Exp1 (Strong Linear) with {val1:.3f}')
print(f'            Exp3 very close: only {(val1 - val3)/val1*100:.1f}% behind')
print()

# Episode Statistics
print('TRAINING EPISODE STATISTICS')
print('-' * 80)

exp1_returns = exp1['episode_returns']
exp2_returns = exp2['episode_returns']
exp3_returns = exp3['episode_returns']

exp1_sharpes = exp1['episode_sharpes']
exp2_sharpes = exp2['episode_sharpes']
exp3_sharpes = exp3['episode_sharpes']

exp1_rewards = exp1['episode_rewards']
exp2_rewards = exp2['episode_rewards']
exp3_rewards = exp3['episode_rewards']

print('Mean Episode Return (Training):')
m1 = mean(exp1_returns)
m2 = mean(exp2_returns)
m3 = mean(exp3_returns)
print(f'  Exp1: {m1:+.3f}%  |  Exp2: {m2:+.3f}%  |  Exp3: {m3:+.3f}%')
print()

print('% Episodes with Positive Returns:')
pos1 = count_positive(exp1_returns)
pos2 = count_positive(exp2_returns)
pos3 = count_positive(exp3_returns)
print(f'  Exp1: {pos1}/200 ({pos1/2:.0f}%)  |  Exp2: {pos2}/200 ({pos2/2:.0f}%)  |  Exp3: {pos3}/200 ({pos3/2:.0f}%)')
print()

print('% Episodes with Positive Sharpe:')
sharpe1 = count_positive(exp1_sharpes)
sharpe2 = count_positive(exp2_sharpes)
sharpe3 = count_positive(exp3_sharpes)
print(f'  Exp1: {sharpe1}/200 ({sharpe1/2:.0f}%)  |  Exp2: {sharpe2}/200 ({sharpe2/2:.0f}%)  |  Exp3: {sharpe3}/200 ({sharpe3/2:.0f}%)')
print()

print('Mean Training Reward (arbitrary units):')
r1 = mean(exp1_rewards)
r2 = mean(exp2_rewards)
r3 = mean(exp3_rewards)
print(f'  Exp1: {r1:.1f}  |  Exp2: {r2:.1f}  |  Exp3: {r3:.1f}')
print()

# Volatility analysis
print('VOLATILITY ANALYSIS')
print('-' * 80)
print('Return Volatility (Training Episodes):')
s1 = std(exp1_returns)
s2 = std(exp2_returns)
s3 = std(exp3_returns)
print(f'  Exp1: {s1:.3f}%  |  Exp2: {s2:.3f}%  |  Exp3: {s3:.3f}%')
print()

# Compare to original weak penalty version
print('='*80)
print('COMPARISON TO ORIGINAL WEAK PENALTY VERSION')
print('='*80)
print('Original (sensitivity=20):  Validation Sharpe = -3.609  ‚ùå GAMBLING')
print(f'Exp1 (sensitivity=100):     Validation Sharpe = {val1:+.3f}  ‚úÖ STABLE')
print(f'Exp2 (exp decay, k=50):     Validation Sharpe = {val2:+.3f}  ‚úÖ STABLE')
print(f'Exp3 (direct Sharpe):       Validation Sharpe = {val3:+.3f}  ‚úÖ STABLE')
print()
print('üéâ ALL THREE APPROACHES SUCCESSFULLY PREVENTED GAMBLING BEHAVIOR!')
print()

# Recommendation
print('='*80)
print('RECOMMENDATION')
print('='*80)
print()
print('Exp1 (Strong Linear Penalty) and Exp3 (Direct Sharpe) performed nearly identically.')
print(f'  ‚Ä¢ Exp1: {val1:.3f} validation Sharpe')
print(f'  ‚Ä¢ Exp3: {val3:.3f} validation Sharpe')
diff_pct = abs(val1 - val3)/val1*100
print(f'  ‚Ä¢ Difference: only {abs(val1 - val3):.3f} ({diff_pct:.1f}%)')
print()
print('üèÜ RECOMMEND: Exp3 (Direct Sharpe Optimization)')
print()
print('Reasoning:')
print('  1. More theoretically grounded: directly optimizes the metric we evaluate on')
print('  2. Performance virtually identical to Exp1 (+0.480 vs +0.490)')
print('  3. No need to manually tune volatility sensitivity hyperparameter')
print('  4. Should generalize better to different market regimes')
print()
print('Next Steps:')
print('  1. Use Sharpe-direct reward for training regime-specialist agents')
print('  2. Train bull/bear/sideways market specialists')
print('  3. Evaluate comprehensive performance across all regimes')
print('='*80)
